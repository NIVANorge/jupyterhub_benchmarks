{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import corner\n",
    "import emcee\n",
    "import lmfit\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JupyterHub benchmarking\n",
    "\n",
    "## Test 6: Non-linear optimisation and curve fitting\n",
    "\n",
    "**Based on one of the old DSToolkit tutorial notebooks**.\n",
    "\n",
    "In this tutorial, we'll explore a synthetic example fitting the **double exponential** function:\n",
    "\n",
    "$$y = a_1 exp \\left(\\frac{-x}{t_1}\\right) + a_2 exp \\left(-\\frac{(x - 0.1)}{t_2}\\right)$$\n",
    "\n",
    "This is notoriously difficult to fit due to very sensitive parameter co-variances. It's an interesting test case because, even with just 4 parameters, it's surprisingly hard to optimise.\n",
    "\n",
    "## 1. Generate test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deterministic_model(params, x, obs=None):\n",
    "    \"\"\"Double exponential model. If no observations are passed, returns the model\n",
    "    output. Otherwise, returns the residuals.\n",
    "    \"\"\"\n",
    "    v = params.valuesdict()\n",
    "    model = v[\"a1\"] * np.exp(-x / v[\"t1\"]) + v[\"a2\"] * np.exp(-(x - 0.1) / v[\"t2\"])\n",
    "\n",
    "    if obs is None:\n",
    "        return model\n",
    "\n",
    "    return model - obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, I've included an explicit **heteroscedastic** error term (i.e. $\\sigma_{\\epsilon} \\propto y$), as this is relevant to the other tutorial on catchement modelling using [Mobius](https://github.com/NIVANorge/Mobius)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of data points\n",
    "n_pts = 10000\n",
    "\n",
    "# Fake 'true' params\n",
    "true_params = lmfit.Parameters()\n",
    "true_params.add(\"a1\", value=3)\n",
    "true_params.add(\"t1\", value=2)\n",
    "true_params.add(\"a2\", value=-5)\n",
    "true_params.add(\"t2\", value=10)\n",
    "true_params.add(\"m\", value=0.05)\n",
    "\n",
    "# Fake data\n",
    "np.random.seed(0)\n",
    "x = np.linspace(1, 10, n_pts)\n",
    "y = deterministic_model(true_params, x)\n",
    "m = true_params[\"m\"].value\n",
    "y += np.random.normal(scale=m * np.abs(y), size=n_pts)\n",
    "\n",
    "# Plot\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, y)\n",
    "plt.title(\"Synthetic raw data to fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Optimise\n",
    "\n",
    "We start by creating a `Parameters` object and guessing some initial values for the **deterministic** model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define initial params for optimisation\n",
    "# Ignore error term at this stage\n",
    "params = lmfit.Parameters()\n",
    "params.add(\"a1\", value=4.0)\n",
    "params.add(\"t1\", value=3.0)\n",
    "params.add(\"a2\", value=4.0)\n",
    "params.add(\"t2\", value=3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below uses a two-step optimisation procedure for better results. Note that all these optimisers are performing least squares minimisation of the residuals, as returned by the `deterministic_model()` function (defined above). The great thing about this approch is that your deterministic model can be ***anything***. Here, we just have a \"toy\" Python function, but in the Mobius tutorial the deterministic Python function calls a catchment model (SimplyP), which is coded in C++. As long as your function returns the residuals (i.e. simulated - observed), the rest of the optimisation and MCMC code is basically unchanged, which saves a huge amount of time and effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create minimisers\n",
    "mi = lmfit.Minimizer(deterministic_model, params, fcn_args=(x, y), nan_policy=\"omit\")\n",
    "\n",
    "# First solve with Nelder-Mead (more robust than LM)\n",
    "res = mi.minimize(method=\"nelder\")\n",
    "\n",
    "# Then solve with Levenberg-Marquardt using the Nelder-Mead solution\n",
    "# as a starting point\n",
    "res = mi.minimize(method=\"leastsq\", params=res.params)\n",
    "\n",
    "# Robustly estimate CI\n",
    "ci = lmfit.conf_interval(\n",
    "    mi,\n",
    "    res,\n",
    "    sigmas=[\n",
    "        0.95,\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Calculate final result\n",
    "final = res.residual + y  # Residual was (model - y), so this is an\n",
    "# easy way of getting back to 'model'\n",
    "\n",
    "# Report\n",
    "lmfit.report_fit(res)\n",
    "print(\"\\n--------------------------\\n\")\n",
    "lmfit.printfuncs.report_ci(ci)\n",
    "\n",
    "# Plot\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, y)\n",
    "plt.plot(x, final, \"k\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Together, the optimisers do a reasonable job of recovering the \"true\" parameters, despite the very strong correlations.\n",
    "\n",
    "## 3. MCMC\n",
    "\n",
    "To get a more comprehensive understanding of the model parameters and their interactions, we can use Markov chain Monte Carlo (MCMC) sampling, optionally including \"parallel tempering\". If you're new to this topic, there's an introduction [here](http://jamessample.github.io/enviro_mod_notes/).\n",
    "\n",
    "First, we define a **log-likelihood** function, which in this case is also our **log-posterior** (since we're using uniform priors). We also need to add the **stochastic** error parameter, `m`, to the deterministic parameter set we created above. Note that the error term gets added to the `params` of the *optimiser result* object (`res`), and not the original `params` object, because we want to start the MCMC from near the optimised location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lnprob(params, x, y):\n",
    "    \"\"\"Log-likelihood for simple Gaussian error structure. Because\n",
    "    we're using uniform (improper) priors, this is also the\n",
    "    log-posterior.\n",
    "    \"\"\"\n",
    "    # Unpack sigma\n",
    "    m = params[\"m\"].value\n",
    "    model = deterministic_model(params, x)\n",
    "    sigma = m * np.abs(model)\n",
    "\n",
    "    # Calculate log likelihood\n",
    "    ll = np.sum(norm(model, sigma).logpdf(y))\n",
    "\n",
    "    if np.isfinite(ll):\n",
    "        return ll\n",
    "    return -np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to include sigma as an additional parameter in the MCMC\n",
    "# Start from 'best fit' params found above\n",
    "res.params.add(\"m\", value=0.1, min=0.001, max=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    start = time.time()\n",
    "\n",
    "    # Run emcee sampler\n",
    "    mi_mcmc = lmfit.Minimizer(lnprob, res.params, fcn_args=(x, y), nan_policy=\"omit\")\n",
    "    result = mi_mcmc.emcee(\n",
    "        params=mi.params,\n",
    "        burn=500,\n",
    "        steps=1000,\n",
    "        nwalkers=50,\n",
    "        thin=20,\n",
    "        ntemps=1,  # > 1 perform for Parallel Tempering\n",
    "        float_behavior=\"posterior\",  # lnprob returns a float representing log-posterior prob\n",
    "    )\n",
    "\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    print(f\"{elapsed:.1f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
